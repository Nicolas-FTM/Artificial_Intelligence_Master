{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktFg2jj3jTE6"
      },
      "source": [
        "# ACTIVIDAD DE CLASIFICACIÓN DE TEXTO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ-OuW5DiLJs"
      },
      "source": [
        "En esta actividad vamos a trabajar en clasificar textos. Se recorrerá todo el proceso desde traer el dataset hasta proceder a dicha clasificación. Durante la actividad se llevarán a cabo muchos procesos como la creación de un vocabulario, el uso de embeddings y la creación de modelos.\n",
        "\n",
        "Las cuestiones presentes en esta actividad están basadas en un Notebook creado por François Chollet, uno de los creadores de Keras y autor del libro \"Deep Learning with Python\". \n",
        "\n",
        "En este Notebook se trabaja con el dataset \"Newsgroup20\" que contiene aproximadamente 20000 mensajes que pertenecen a 20 categorías diferentes.\n",
        "\n",
        "El objetivo es entender los conceptos que se trabajan y ser capaz de hacer pequeñas experimentaciones para mejorar el Notebook creado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hytURWLLjZvT"
      },
      "source": [
        "# Librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DbxRuvOwkzSs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXfYbCflkQYy"
      },
      "source": [
        "# Descarga de Datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "e-1ZhOf3lB_A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12fb464e-1309-4386-894f-1c0b62bdfbf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\n",
            "17329808/17329808 [==============================] - 17s 1us/step\n"
          ]
        }
      ],
      "source": [
        "data_path = keras.utils.get_file(\n",
        "    \"news20.tar.gz\",\n",
        "    \"http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\",\n",
        "    untar=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3ygvoWhlCYj",
        "outputId": "012f25d9-6189-4271-82fa-08cab1f21937"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of directories: 20\n",
            "Directory names: ['talk.politics.misc', 'misc.forsale', 'rec.autos', 'sci.med', 'rec.sport.hockey', 'rec.sport.baseball', 'comp.graphics', 'rec.motorcycles', 'sci.crypt', 'talk.politics.mideast', 'alt.atheism', 'talk.politics.guns', 'comp.sys.mac.hardware', 'comp.os.ms-windows.misc', 'sci.space', 'sci.electronics', 'comp.sys.ibm.pc.hardware', 'soc.religion.christian', 'comp.windows.x', 'talk.religion.misc']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "#Estructura de directorios del dataset\n",
        "data_dir = pathlib.Path(data_path).parent / \"20_newsgroup\"\n",
        "dirnames = os.listdir(data_dir)\n",
        "print(\"Number of directories:\", len(dirnames))\n",
        "print(\"Directory names:\", dirnames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxkoPI6As-hE",
        "outputId": "16fe9c15-51b1-448e-aed4-993c64826378"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/.keras/datasets/20_newsgroup\n"
          ]
        }
      ],
      "source": [
        "print(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG8rjgOFlcaV",
        "outputId": "70a1a568-08fc-4182-bd86-bd60c6f1a68d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in comp.graphics: 1000\n",
            "Some example filenames: ['38723', '38769', '38256', '38932', '38250']\n"
          ]
        }
      ],
      "source": [
        "#Algunos archivos de la categoria \"com.graphics\"\n",
        "fnames = os.listdir(data_dir / \"comp.graphics\")\n",
        "print(\"Number of files in comp.graphics:\", len(fnames))\n",
        "print(\"Some example filenames:\", fnames[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ox6s6z9lgps",
        "outputId": "94ff1d2b-1f66-4ddd-db63-e7f164e10057"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!noc.near.net!hri.com!enterpoop.mit.edu!gatech!howland.reston.ans.net!torn!nott!bnrgate!bnr.co.uk!uknet!warwick!bham!ibm3090.bham.ac.uk!SITUNAYA\n",
            "From: SITUNAYA@IBM3090.BHAM.AC.UK\n",
            "Newsgroups: comp.graphics\n",
            "Subject: Best FTP Viewer please.\n",
            "Date: 28 Apr 93 11:50:33 BST\n",
            "Organization: The University of Birmingham, United Kingdom\n",
            "Lines: 5\n",
            "Message-ID: <935028115033@ibm3090.bham.ac.uk>\n",
            "NNTP-Posting-Host: ibm3090.bham.ac.uk\n",
            "\n",
            "==============================================================================\n",
            "Could someone please tell me the Best FTP'able viewer available for MSDOS\n",
            "I am running a 486 33mhz with SVGA monitor.\n",
            "I need to look at gifs mainly and it would be advantageous if it ran\n",
            "under windows...........thanks\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Ejemplo de un texto de la categoría \"com.graphics\"\n",
        "print(open(data_dir / \"comp.graphics\" / \"38891\").read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUbbjI8plaG0",
        "outputId": "701b8d9c-9fc6-45dc-fbe9-60cabd48c413"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in talk.politics.misc: 1000\n",
            "Some example filenames: ['179061', '179094', '176919', '178414', '178512']\n"
          ]
        }
      ],
      "source": [
        "#Algunos archivos de la categoria \"talk.politics.misc\"\n",
        "fnames = os.listdir(data_dir / \"talk.politics.misc\")\n",
        "print(\"Number of files in talk.politics.misc:\", len(fnames))\n",
        "print(\"Some example filenames:\", fnames[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izZGWhpklCbI",
        "outputId": "6da8d6b1-5cb5-41e2-c834-aa3e1b33bd12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Xref: cantaloupe.srv.cs.cmu.edu talk.politics.guns:54219 talk.politics.misc:178463\n",
            "Newsgroups: talk.politics.guns,talk.politics.misc\n",
            "Path: cantaloupe.srv.cs.cmu.edu!magnesium.club.cc.cmu.edu!news.sei.cmu.edu!cis.ohio-state.edu!magnus.acs.ohio-state.edu!usenet.ins.cwru.edu!agate!spool.mu.edu!darwin.sura.net!martha.utcc.utk.edu!FRANKENSTEIN.CE.UTK.EDU!VEAL\n",
            "From: VEAL@utkvm1.utk.edu (David Veal)\n",
            "Subject: Re: Proof of the Viability of Gun Control\n",
            "Message-ID: <VEAL.749.735192116@utkvm1.utk.edu>\n",
            "Lines: 21\n",
            "Sender: usenet@martha.utcc.utk.edu (USENET News System)\n",
            "Organization: University of Tennessee Division of Continuing Education\n",
            "References: <1qpbqd$ntl@access.digex.net> <C5otvp.ItL@magpie.linknet.com>\n",
            "Date: Mon, 19 Apr 1993 04:01:56 GMT\n",
            "\n",
            "[alt.drugs and alt.conspiracy removed from newsgroups line.]\n",
            "\n",
            "In article <C5otvp.ItL@magpie.linknet.com> neal@magpie.linknet.com (Neal) writes:\n",
            "\n",
            ">   Once the National Guard has been called into federal service,\n",
            ">it is under the command of the present. Tha National Guard, though\n",
            ">defined as the \"Militia\" in the statutes, is actually a reserve component\n",
            ">of the United State Army, and was formed pursuant to the power of Congress\n",
            ">to raise and support Armies.\n",
            "\n",
            "       That's the really cute thing about saying the 2nd amendment\n",
            "only covers the national guard, because that would mean that it\n",
            "essentially prohibits the federal government from disarming a branch\n",
            "of the federal government.\n",
            "\n",
            "       Sounds like a real limit to federal power to me.\n",
            "------------------------------------------------------------------------\n",
            "David Veal Univ. of Tenn. Div. of Cont. Education Info. Services Group\n",
            "PA146008@utkvm1.utk.edu - \"I still remember the way you laughed, the day\n",
            "your pushed me down the elevator shaft;  I'm beginning to think you don't\n",
            "love me anymore.\" - \"Weird Al\"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Ejemplo de un texto de la categoría \"talk.politics.misc\"\n",
        "print(open(data_dir / \"talk.politics.misc\" / \"178463\").read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33Ay5U6blCd1",
        "outputId": "7f219fe6-ce8c-4c8a-a662-034f628aa7c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing alt.atheism, 1000 files found\n",
            "#####################################################\n",
            "['Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:54169 talk.religion.misc:84359 talk.origins:41165', 'Newsgroups: alt.atheism,talk.religion.misc,talk.origins', 'Path: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!noc.near.net!howland.reston.ans.net!zaphod.mps.ohio-state.edu!uwm.edu!linac!att!att!allegra!ulysses!ulysses.att.com!mls', 'From: mls@ulysses.att.com (Michael L. Siemon)', 'Subject: Re: Ancient references to Christianity (was: Albert Sabin)', 'Summary: Use of historical sources ...', 'Message-ID: <1993Apr26.143715.22173@ulysses.att.com>', 'Date: Mon, 26 Apr 1993 14:37:15 GMT', 'References: <1r67ruINNmle@ctron-news.ctron.com> <C5ztJu.FKx@news.cso.uiuc.edu> <C62B7n.6B4@news.cso.uiuc.edu>', 'Organization: AT&T Bell Labs, Murray Hill, NJ, USA']\n",
            "----------------------------------------------\n",
            "['Lines: 83', '', 'In article <C62B7n.6B4@news.cso.uiuc.edu> cobb@alexia.lis.uiuc.edu', '(Mike Cobb) writes:', '', '>In <1ren9a$94q@morrow.stanford.edu> salem@pangea.Stanford.EDU (Bruce Salem) ', '>writes:', '', '>>In article <C5ztJu.FKx@news.cso.uiuc.edu> cobb@alexia.lis.uiuc.edu', '(Mike Cobb) writes:']\n",
            "#####################################################\n",
            "Processing comp.graphics, 1000 files found\n",
            "Processing comp.os.ms-windows.misc, 1000 files found\n",
            "Processing comp.sys.ibm.pc.hardware, 1000 files found\n",
            "Processing comp.sys.mac.hardware, 1000 files found\n",
            "Processing comp.windows.x, 1000 files found\n",
            "Processing misc.forsale, 1000 files found\n",
            "Processing rec.autos, 1000 files found\n",
            "Processing rec.motorcycles, 1000 files found\n",
            "Processing rec.sport.baseball, 1000 files found\n",
            "Processing rec.sport.hockey, 1000 files found\n",
            "Processing sci.crypt, 1000 files found\n",
            "Processing sci.electronics, 1000 files found\n",
            "Processing sci.med, 1000 files found\n",
            "Processing sci.space, 1000 files found\n",
            "Processing soc.religion.christian, 997 files found\n",
            "Processing talk.politics.guns, 1000 files found\n",
            "Processing talk.politics.mideast, 1000 files found\n",
            "Processing talk.politics.misc, 1000 files found\n",
            "Processing talk.religion.misc, 1000 files found\n",
            "Classes: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
            "Number of samples: 19997\n"
          ]
        }
      ],
      "source": [
        "samples = []\n",
        "labels = []\n",
        "class_names = []\n",
        "class_index = 0\n",
        "aux = True\n",
        "for dirname in sorted(os.listdir(data_dir)):\n",
        "    class_names.append(dirname)\n",
        "    dirpath = data_dir / dirname\n",
        "    fnames = os.listdir(dirpath)\n",
        "    print(\"Processing %s, %d files found\" % (dirname, len(fnames)))\n",
        "    for fname in fnames:\n",
        "        fpath = dirpath / fname\n",
        "        f = open(fpath, encoding=\"latin-1\")\n",
        "        content = f.read()\n",
        "        lines = content.split(\"\\n\")\n",
        "        if aux:\n",
        "          print(\"#####################################################\")\n",
        "          print(lines[0:10])\n",
        "          print(\"----------------------------------------------\")\n",
        "          print(lines[10:20])\n",
        "          aux=False\n",
        "          print(\"#####################################################\")\n",
        "\n",
        "        lines = lines[10:] #Aquí es donde se descartan las 10 primeras lineas de cada archivo\n",
        "        content = \"\\n\".join(lines)\n",
        "        samples.append(content)\n",
        "        labels.append(class_index)\n",
        "    class_index += 1\n",
        "\n",
        "print(\"Classes:\", class_names)\n",
        "print(\"Number of samples:\", len(samples))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2pmvE6gMcxT"
      },
      "source": [
        "# Mezclando los datos para separarlos en Traning y Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "DYX7x-k_lCgZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ef97f32-d039-47e5-a0c8-043034f26018"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Muestra del training:\n",
            "In article <1993Apr20.151818.4319@samba.oit.unc.edu> Scott.Marks@launchpad.unc.edu (Scott Marks) writes:\n",
            ">>And of course, Mike Ramsey was (at one time) the captain in Buffalo prior to\n",
            ">>being traded to Pittsburgh.  Currently, the Penguins have 3 former captains\n",
            ">>and 1 real captain (Lemieux) playing for them.  They rotate the A's during the\n",
            ">>season (and even the C while Mario was out).  Even Troy Loney has worn the C\n",
            ">>for the Pens.\n",
            ">\n",
            "\n",
            "I think that Mike Foligno was the captain of the Sabres when he\n",
            "got traded to the Leafs. Also, wasn't Rick Vaive the captain of\n",
            "the Leafs when he got traded to Chicago (with Steve Thomas for\n",
            "Ed Olcyzk and someone). Speaking of the Leafs, I believe that\n",
            "Darryl Sittler was their captain (he'd torn the \"C\" off his\n",
            "jersey but I think he re-claimed the captaincy later on) when he\n",
            "was traded to the Flyers.\n",
            "\n",
            "Oh yeah, of course, Gretzky was the captain of the Oilers before\n",
            "he was traded wasn't he? \n",
            "\n",
            "Gary\n",
            "\n",
            "#################################################\n",
            "Muestra del test:\n",
            "Reply-To: ch981@cleveland.Freenet.Edu (Tony Alicea)\n",
            "NNTP-Posting-Host: hela.ins.cwru.edu\n",
            "\n",
            "\n",
            "In a previous article, sandvik@newton.apple.com (Kent Sandvik) says:\n",
            "\n",
            ">Well, it depends how you look at it. If you are interested I might\n",
            ">find out what the latest status is in this legal battle.\n",
            ">Kent\n",
            ">\n",
            "\tPlease do! And if you don't want to post it here, email to me\n",
            ":-) I don't know how this discussion is appreciated here. I hate\n",
            "'invading' newsgroups with themes of limited interest :-)\n",
            "\n",
            "Tony\n",
            "\n",
            "\n",
            "#################################################\n",
            "Etiqueta del training:\n",
            "10\n",
            "#################################################\n",
            "Etiqueta del test:\n",
            "19\n"
          ]
        }
      ],
      "source": [
        "# Shuffle the data\n",
        "seed = 1337\n",
        "rng = np.random.RandomState(seed)\n",
        "rng.shuffle(samples)\n",
        "rng = np.random.RandomState(seed)\n",
        "rng.shuffle(labels)\n",
        "\n",
        "# Extract a training & validation split\n",
        "validation_split = 0.2\n",
        "num_validation_samples = int(validation_split * len(samples))\n",
        "train_samples = samples[:-num_validation_samples] \n",
        "val_samples = samples[-num_validation_samples:] \n",
        "train_labels = labels[:-num_validation_samples]\n",
        "val_labels = labels[-num_validation_samples:]\n",
        "\n",
        "print(\"Muestra del training:\")\n",
        "print(train_samples[0])\n",
        "print(\"#################################################\")\n",
        "print(\"Muestra del test:\")\n",
        "print(val_samples[0])\n",
        "print(\"#################################################\")\n",
        "print(\"Etiqueta del training:\")\n",
        "print(train_labels[0])\n",
        "print(\"#################################################\")\n",
        "print(\"Etiqueta del test:\")\n",
        "print(val_labels[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IktOtKfpNx8E"
      },
      "source": [
        "# Tokenización de las palabras con TextVectorization "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "QjHgQPX8lCjO"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\n",
        "vectorizer.adapt(text_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIWC37s5smZ4",
        "outputId": "edceedc3-96f6-42af-ef57-ba53f0ea5eb4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'the', 'to', 'of']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "vectorizer.get_vocabulary()[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vit8TPqTvmwS",
        "outputId": "e5a99dbe-c7a5-4821-91ee-66fd1b3ccc3f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20000"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "len(vectorizer.get_vocabulary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O-FXA9wPVkg"
      },
      "source": [
        "# Viendo la salida de Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rseIF0fLmyJ0",
        "outputId": "a4eed5af-7780-4f4a-c487-3a25954f8ea6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   2, 3454, 1659,   15,    2, 8034])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "output = vectorizer([[\"the cat sat on the mat\"]])\n",
        "output.numpy()[0, :6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wsr4AQtBFArV",
        "outputId": "83851811-db93-4356-e809-b39231edfcda"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 200), dtype=int64, numpy=\n",
              "array([[   2, 3454, 1659,   15,    2, 8034,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0]])>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "SL5ag8UamzwL"
      },
      "outputs": [],
      "source": [
        "voc = vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08v8SKcsn3lf",
        "outputId": "9a071d04-ceed-436d-b094-2b5d05be55fe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3454, 1659, 15, 2, 8034]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "test = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "[word_index[w] for w in test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eBhadrvOTNZ"
      },
      "source": [
        "# Tokenización de los datos de entrenamiento y validación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "W26LUr2dKTOj"
      },
      "outputs": [],
      "source": [
        "x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
        "x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
        "\n",
        "y_train = np.array(train_labels)\n",
        "y_val = np.array(val_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3QVIb84Olda"
      },
      "source": [
        "# Creación y entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilizando el tokenizador de spacy, que ya conoces, calcula el número promedio de tokens de una muestra de 15 ficheros de la categoría ‘com.graphics’. Indica el código utilizado y el resultado obtenido"
      ],
      "metadata": {
        "id": "1CimaiIXwHRY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "B9VxI-i69cdB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bf080e7-433d-43c4-8c9e-5b972d6653b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39049\n",
            "38827\n",
            "38393\n",
            "38099\n",
            "38403\n",
            "38708\n",
            "38340\n",
            "38345\n",
            "38405\n",
            "38592\n",
            "38928\n",
            "38958\n",
            "39662\n",
            "38934\n",
            "38462\n"
          ]
        }
      ],
      "source": [
        "#Primero vamos a ver como se llaman los 15 archivos de com.graphics\n",
        "import os\n",
        "import random\n",
        "\n",
        "archivos = os.listdir(data_dir/ \"comp.graphics\")\n",
        "\n",
        "\n",
        "# Seleccionar aleatoriamente 15 archivos\n",
        "archivos_seleccionados = random.sample(archivos, 15)\n",
        "\n",
        "# Mostrar los nombres de los archivos en pantalla\n",
        "for archivo in archivos_seleccionados:\n",
        "    print(archivo)\n",
        "\n",
        "# Guardar los nombres de los archivos en una variable\n",
        "nombres_archivos = archivos_seleccionados\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import Doc\n",
        "from pathlib import Path\n",
        "\n",
        "# Cargar el modelo de idioma en inglés\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Lista de archivos de la categoría 'comp.graphics'\n",
        "archivos = nombres_archivos\n",
        "\n",
        "# Variable para almacenar el número total de tokens\n",
        "total_tokens = 0\n",
        "\n",
        "# Procesar cada archivo\n",
        "for archivo in archivos:\n",
        "    # Construir la ruta completa del archivo\n",
        "    ruta_archivo = data_dir / \"comp.graphics\" / archivo\n",
        "    \n",
        "    # Leer el contenido del archivo\n",
        "    with open(ruta_archivo, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "        texto = file.read()\n",
        "    \n",
        "    # Crear un objeto Doc utilizando el tokenizador de spaCy\n",
        "    doc = Doc(nlp.vocab, words=texto.split())\n",
        "    \n",
        "    # Obtener el número de tokens en el documento\n",
        "    num_tokens = len(doc)\n",
        "    \n",
        "    # Agregar el número de tokens al total\n",
        "    total_tokens += num_tokens\n",
        "    \n",
        "    # Mostrar la longitud del archivo\n",
        "    print(\"Longitud del archivo\", archivo, \":\", num_tokens)\n",
        "\n",
        "# Calcular el número promedio de tokens\n",
        "promedio_tokens = total_tokens / len(archivos)\n",
        "\n",
        "# Imprimir el resultado\n",
        "print(\"Número promedio de tokens:\", promedio_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8N-kg3lxCM4",
        "outputId": "f032a7a4-d3ae-4c86-e9d7-1d51ca0bbbbc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longitud del archivo 39049 : 145\n",
            "Longitud del archivo 38827 : 176\n",
            "Longitud del archivo 38393 : 124\n",
            "Longitud del archivo 38099 : 148\n",
            "Longitud del archivo 38403 : 9200\n",
            "Longitud del archivo 38708 : 104\n",
            "Longitud del archivo 38340 : 206\n",
            "Longitud del archivo 38345 : 314\n",
            "Longitud del archivo 38405 : 100\n",
            "Longitud del archivo 38592 : 132\n",
            "Longitud del archivo 38928 : 190\n",
            "Longitud del archivo 38958 : 99\n",
            "Longitud del archivo 39662 : 65\n",
            "Longitud del archivo 38934 : 74\n",
            "Longitud del archivo 38462 : 899\n",
            "Número promedio de tokens: 798.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# El código proporcionado lee los ficheros uno a uno y, antes de generar el catálogo de datos de entrenamiento y validación, descarta las 10 primeras líneas de cada fichero. ¿Cuál es el trozo de código en el que se realiza dicho descarte?, ¿por qué crees que se descartan dichas líneas?, ¿por qué 10 y no otro número? (1 punto)"
      ],
      "metadata": {
        "id": "SkbvvVJf0oUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "samples = []\n",
        "labels = []\n",
        "class_names = []\n",
        "class_index = 0\n",
        "for dirname in sorted(os.listdir(data_dir)):\n",
        "    class_names.append(dirname)\n",
        "    dirpath = data_dir / dirname\n",
        "    fnames = os.listdir(dirpath)\n",
        "    print(\"Processing %s, %d files found\" % (dirname, len(fnames)))\n",
        "    for fname in fnames:\n",
        "        fpath = dirpath / fname\n",
        "        f = open(fpath, encoding=\"latin-1\")\n",
        "        content = f.read()\n",
        "        lines = content.split(\"\\n\")\n",
        "        lines = lines[10:] #Aquí es donde se descartan las 10 primeras lineas de cada archivo\n",
        "        content = \"\\n\".join(lines)\n",
        "        samples.append(content)\n",
        "        labels.append(class_index)\n",
        "    class_index += 1\n",
        "\n",
        "print(\"Classes:\", class_names)\n",
        "print(\"Number of samples:\", len(samples))"
      ],
      "metadata": {
        "id": "Gs1zW_oQ0ups",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a65cddc4-24aa-4d29-af9c-40fefb17c48b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing alt.atheism, 1000 files found\n",
            "Processing comp.graphics, 1000 files found\n",
            "Processing comp.os.ms-windows.misc, 1000 files found\n",
            "Processing comp.sys.ibm.pc.hardware, 1000 files found\n",
            "Processing comp.sys.mac.hardware, 1000 files found\n",
            "Processing comp.windows.x, 1000 files found\n",
            "Processing misc.forsale, 1000 files found\n",
            "Processing rec.autos, 1000 files found\n",
            "Processing rec.motorcycles, 1000 files found\n",
            "Processing rec.sport.baseball, 1000 files found\n",
            "Processing rec.sport.hockey, 1000 files found\n",
            "Processing sci.crypt, 1000 files found\n",
            "Processing sci.electronics, 1000 files found\n",
            "Processing sci.med, 1000 files found\n",
            "Processing sci.space, 1000 files found\n",
            "Processing soc.religion.christian, 997 files found\n",
            "Processing talk.politics.guns, 1000 files found\n",
            "Processing talk.politics.mideast, 1000 files found\n",
            "Processing talk.politics.misc, 1000 files found\n",
            "Processing talk.religion.misc, 1000 files found\n",
            "Classes: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
            "Number of samples: 19997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "En el codigo se decartan las 10 primeras líneas con (lines = lines[10:]), asumiendo que son metadatos o encabezados que deben omitirse.\n",
        "\n"
      ],
      "metadata": {
        "id": "IEfMXtaq0vuq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ¿Qué se controla con el parámetro 'validation_split'?, ¿por qué se ha elegido ese valor?, ¿qué ocurre si lo modificas? "
      ],
      "metadata": {
        "id": "25jOHwNU1dgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the data\n",
        "seed = 1337\n",
        "rng = np.random.RandomState(seed)\n",
        "rng.shuffle(samples)\n",
        "rng = np.random.RandomState(seed)\n",
        "rng.shuffle(labels)\n",
        "\n",
        "# Extract a training & validation split\n",
        "validation_split = 0.2\n",
        "num_validation_samples = int(validation_split * len(samples))\n",
        "train_samples = samples[:-num_validation_samples]\n",
        "val_samples = samples[-num_validation_samples:]\n",
        "train_labels = labels[:-num_validation_samples]\n",
        "val_labels = labels[-num_validation_samples:]"
      ],
      "metadata": {
        "id": "t5oVjC0u1f1q"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "El código está utilizando el valor validation_split para controlar la proporción de muestras que se utilizarán como datos de validación en relación al total de muestras. El número específico 0.2 utilizado en este código indica que se asignará el 20% de las muestras como datos de validación, mientras que el 80% restante se utilizará como datos de entrenamiento.\n",
        "\n",
        "La elección de este número puede variar según el conjunto de datos y los requisitos del problema. En muchos casos, se utiliza una división del 70-30 (entrenamiento-validación) o 80-20. En este caso, se ha elegido una división del 80-20.\n"
      ],
      "metadata": {
        "id": "GzHrGhuu1i_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imprime por pantalla un ejemplo (es decir, un elemento del array) de ‘train_samples’, ‘val_samples’, ‘train_labels’ y ‘val_labels’. A tenor de las etiquetas que se utilizan, ¿qué tarea crees que se está intentando entrenar? "
      ],
      "metadata": {
        "id": "6CJSvqEh1mrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Imprimir un ejemplo de train_samples\n",
        "print(\"Ejemplo de train_samples:\")\n",
        "indice_train = random.randint(0, len(train_samples) - 1)\n",
        "print(train_samples[indice_train])\n",
        "print()\n",
        "\n",
        "# Imprimir un ejemplo de val_samples\n",
        "print(\"Ejemplo de val_samples:\")\n",
        "indice_val = random.randint(0, len(val_samples) - 1)\n",
        "print(val_samples[indice_val])\n",
        "print()\n",
        "\n",
        "# Imprimir un ejemplo de train_labels\n",
        "print(\"Ejemplo de train_labels:\")\n",
        "indice_train_labels = indice_train\n",
        "print(train_labels[indice_train_labels])\n",
        "print()\n",
        "\n",
        "# Imprimir un ejemplo de val_labels\n",
        "print(\"Ejemplo de val_labels:\")\n",
        "indice_val_labels = indice_val\n",
        "print(val_labels[indice_val_labels])\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBnfqwA62HRE",
        "outputId": "ed705a89-f1e6-4086-8638-c0718c1e7ae8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ejemplo de train_samples:\n",
            "\n",
            "DAK988S@vma.smsu.edu writes:\n",
            ">No....Hal McRae is the worst manager in baseball.\n",
            "\n",
            "I haven't seen enough Royals' games to judge his tactics, so you may have\n",
            "a point here.  But:\n",
            "\n",
            ">I've never seen a guy who can waste talent like he can.  One of the best\n",
            ">raw-talent staffs in the league, and he's still finding a way to lose.\n",
            "\n",
            "IMO, the Royals don't have a chance to win the pennant even if McRae\n",
            "suddenly began channeling for John McGraw.  OK, they have some decent\n",
            "pitchers.  But when your offense consists of bums like Gagne and Lind\n",
            "and McReynolds and McRae and an over-the-hill Brett, you're not going\n",
            "to finish .500 unless McGraw brings Christy Mathewson back with him.\n",
            "\n",
            "I'd say it is hard to evaluate a manager when all of his hitters suck.\n",
            "\n",
            "Bob Davis\trbd@thor.ece.uc.edu\n",
            "\n",
            "\n",
            "Ejemplo de val_samples:\n",
            "\n",
            "In article <GERRY.93Apr21132149@onion.cmu.edu> gerry@cmu.edu (Gerry Roston) writes:\n",
            "> 4th Amendment\n",
            "> The right of the people to be secure in their persons, houses, \n",
            "> papers, and effects, against unreasonable searches and seizures, \n",
            "> shall not be violated; and no warrants shall issue, but upon \n",
            "> probable cause, supported by oath or affirmation, and \n",
            "> particularly describing the place to be searched and the persons \n",
            "> or things to be seized. \n",
            ">\n",
            ">No, a no-knock warrant is in clear violation of the 4th amendment.\n",
            "\n",
            "I guess my news reader deleted the lines of the 4th amendment which deal\n",
            "with no-knock warrants.  How do you deduce that they are in clear violation?\n",
            "Now maybe no-knock warrants SHOULD be illegal.  But until the Supreme Court\n",
            "says so, your own pronouncements on the warrant's constitutionality are just\n",
            "wishful thinking.\n",
            "\n",
            "\n",
            "Ejemplo de train_labels:\n",
            "9\n",
            "\n",
            "Ejemplo de val_labels:\n",
            "19\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Con 'output_sequence_length' se establece un tamaño fijo para la salida de Vectorizer. ¿Por qué se necesita un tamaño fijo, y por qué se ha elegido el valor ‘200’? "
      ],
      "metadata": {
        "id": "IkkYCAph2Rdc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al trabajar con modelos de aprendizaje automático, es común que la entrada y salida tengan una dimensión fija. Esto se debe a que los modelos requieren entradas de tamaño constante para realizar operaciones eficientes y tener una representación consistente de los datos.\n",
        "\n",
        "En el caso específico de 'output_sequence_length', se establece un tamaño fijo para la salida del Vectorizer para garantizar que todas las secuencias de texto tengan la misma longitud en su representación vectorial. Esto es necesario para alimentar adecuadamente los datos a un modelo, ya que generalmente se requiere que todas las entradas tengan la misma dimensión.\n",
        "\n",
        "El valor específico de '200' para 'output_sequence_length' fue elegido probablemente en función de la longitud promedio de las secuencias de texto en el conjunto de datos o consideraciones específicas del problema. Es común seleccionar un valor lo suficientemente grande para abarcar la mayoría de las secuencias, pero no tan grande como para causar problemas de rendimiento o memoria. Es posible que se hayan realizado experimentos previos para determinar que un tamaño de 200 era adecuado para capturar la información relevante en el contexto del problema en cuestión.\n",
        "\n",
        "Es importante tener en cuenta que el valor de 'output_sequence_length' puede variar según el conjunto de datos y el problema específico que se esté abordando. En algunos casos, puede ser necesario ajustar este valor y realizar pruebas para encontrar el tamaño óptimo que funcione mejor para el modelo y los datos específicos. \n"
      ],
      "metadata": {
        "id": "wutkOj9k2qRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers"
      ],
      "metadata": {
        "id": "J3mIGPnK5vq6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "vbUUB_y65upS"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " La clase TransformerBlock define un bloque de la arquitectura Transformer que se puede utilizar como componente en un modelo de Transformer más grande. El bloque del Transformer realiza las siguientes operaciones:\n",
        "\n",
        "Atención multi-cabeza: Utiliza la capa MultiHeadAttention de TensorFlow para aplicar atención multi-cabeza a los datos de entrada. La atención multi-cabeza permite que el bloque capture relaciones y dependencias entre diferentes partes de la secuencia de entrada.\n",
        "\n",
        "Capa feed-forward: Utiliza una capa Dense de TensorFlow para aplicar una transformación no lineal (usualmente una función de activación ReLU) a la salida de la atención multi-cabeza. La capa feed-forward introduce no linealidad y ayuda al bloque a aprender representaciones más complejas de los datos.\n",
        "\n",
        "Normalización: Utiliza las capas LayerNormalization de TensorFlow para realizar la suma residual y la normalización en la salida de la atención multi-cabeza y de la capa feed-forward. La normalización ayuda a estabilizar el aprendizaje y facilita el flujo de los gradientes durante la retropropagación.\n",
        "\n",
        "Dropout: Utiliza las capas Dropout de TensorFlow para aplicar regularización mediante la eliminación aleatoria de conexiones durante el entrenamiento. El dropout ayuda a prevenir el sobreajuste y mejora la generalización del modelo.\n",
        "\n",
        "En conjunto, el bloque del Transformer captura patrones y relaciones en los datos de entrada a través de la atención multi-cabeza, aplica transformaciones no lineales con la capa feed-forward y utiliza normalización y dropout para regularizar y estabilizar el proceso de aprendizaje. Este bloque se puede repetir varias veces en un modelo de Transformer más grande para construir arquitecturas más profundas y complejas que se utilizan en tareas de procesamiento del lenguaje natural y otros dominios."
      ],
      "metadata": {
        "id": "A63W_76jKBKF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "4HIVHglL3oS1"
      },
      "outputs": [],
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La clase TokenAndPositionEmbedding implementa una capa de embedding que agrega información de tokens y posiciones a través de incrustaciones. Esto permite que el modelo capture tanto el contenido (significado) como la estructura posicional de la secuencia de entrada. Al combinar estas dos fuentes de información, el modelo puede comprender mejor las relaciones y dependencias entre los tokens en la secuencia, lo que es especialmente útil en tareas de procesamiento del lenguaje natural y otros dominios donde la estructura posicional es importante para comprender el significado global de la secuencia."
      ],
      "metadata": {
        "id": "n-a0XPqxKlX6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "iWuDTn1u3oS2"
      },
      "outputs": [],
      "source": [
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "num_tokens = len(voc) + 2\n",
        "\n",
        "maxlen = 200 \n",
        "vocab_size = num_tokens\n",
        "\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
        "\n",
        "modeloTransformers = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se definen varios parámetros relacionados con la arquitectura del modelo:\n",
        "\n",
        "embed_dim representa la dimensión de las incrustaciones (embeddings) para cada token en el texto.\n",
        "num_heads especifica el número de cabezas de atención utilizadas en la capa de atención multi-cabeza del Transformer.\n",
        "ff_dim es el tamaño de la capa oculta en la red de retroalimentación (feed-forward) dentro del bloque Transformer.\n",
        "num_tokens es el número total de tokens en el vocabulario, incluyendo tokens especiales (como el token de inicio y el token de padding).\n",
        "Se definen los tamaños de las secuencias de entrada y el tamaño del vocabulario:\n",
        "\n",
        "maxlen es la longitud máxima de las secuencias de entrada (texto). Se establece en 200 tokens en este caso.\n",
        "vocab_size es el tamaño del vocabulario utilizado para la incrustación de tokens y se establece como num_tokens.\n",
        "Se define una capa de entrada (inputs) que espera una secuencia de longitud maxlen de tokens.\n",
        "\n",
        "Se crea una capa de incrustación (embedding_layer) utilizando la clase TokenAndPositionEmbedding que definiste anteriormente. Esta capa combina la información de tokens y posiciones en la secuencia de entrada.\n",
        "\n",
        "La secuencia de entrada se pasa a través de la capa de incrustación (x = embedding_layer(inputs)).\n",
        "\n",
        "Se crea un bloque Transformer (transformer_block) utilizando la clase TransformerBlock que definiste anteriormente. Este bloque aplica la atención multi-cabeza y la red de retroalimentación para capturar relaciones y aprender representaciones más complejas de los datos.\n",
        "\n",
        "La secuencia de incrustaciones se pasa a través del bloque Transformer (x = transformer_block(x)).\n",
        "\n",
        "Se aplica una operación de pooling global promedio (layers.GlobalAveragePooling1D()) para resumir las características de la secuencia en un vector de características de longitud fija.\n",
        "\n",
        "Se aplica una capa de dropout (layers.Dropout(0.1)) para regularizar el modelo.\n",
        "\n",
        "Se agrega una capa densa (layers.Dense(20, activation=\"relu\")) con una función de activación ReLU para introducir no linealidad y aprender representaciones más complejas.\n",
        "\n",
        "Se aplica una capa de dropout adicional.\n",
        "\n",
        "Finalmente, se agrega una capa densa de salida (layers.Dense(len(class_names), activation=\"softmax\")) con una función de activación softmax para obtener las probabilidades de pertenencia a cada clase de salida.\n",
        "\n",
        "En resumen, este código define una arquitectura de modelo basada en Transformer para clasificar texto, que combina capas de incrustación de tokens y posiciones, capas de atención multi-cabeza y capas densas para aprender representaciones y realizar la clasificación."
      ],
      "metadata": {
        "id": "cPEqwmLTLS0e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "2qK7_uQb3oS2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28c3198f-3d19-465c-d8d4-62cc5168cdf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "125/125 [==============================] - 27s 147ms/step - loss: 2.9039 - acc: 0.0949 - val_loss: 2.6165 - val_acc: 0.1968\n",
            "Epoch 2/20\n",
            "125/125 [==============================] - 13s 106ms/step - loss: 2.3960 - acc: 0.2315 - val_loss: 1.9428 - val_acc: 0.4004\n",
            "Epoch 3/20\n",
            "125/125 [==============================] - 12s 93ms/step - loss: 1.8361 - acc: 0.4004 - val_loss: 1.5716 - val_acc: 0.4684\n",
            "Epoch 4/20\n",
            "125/125 [==============================] - 11s 84ms/step - loss: 1.4035 - acc: 0.5424 - val_loss: 1.2118 - val_acc: 0.6049\n",
            "Epoch 5/20\n",
            "125/125 [==============================] - 9s 70ms/step - loss: 1.1372 - acc: 0.6175 - val_loss: 0.9449 - val_acc: 0.7002\n",
            "Epoch 6/20\n",
            "125/125 [==============================] - 6s 46ms/step - loss: 0.9549 - acc: 0.6851 - val_loss: 1.0764 - val_acc: 0.6507\n",
            "Epoch 7/20\n",
            "125/125 [==============================] - 6s 50ms/step - loss: 0.8445 - acc: 0.7209 - val_loss: 0.8700 - val_acc: 0.7247\n",
            "Epoch 8/20\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 0.7454 - acc: 0.7545 - val_loss: 1.0625 - val_acc: 0.6919\n",
            "Epoch 9/20\n",
            "125/125 [==============================] - 6s 45ms/step - loss: 0.6761 - acc: 0.7765 - val_loss: 0.9246 - val_acc: 0.7309\n",
            "Epoch 10/20\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.6140 - acc: 0.8005 - val_loss: 0.8919 - val_acc: 0.7494\n",
            "Epoch 11/20\n",
            "125/125 [==============================] - 5s 37ms/step - loss: 0.5574 - acc: 0.8184 - val_loss: 1.0356 - val_acc: 0.7112\n",
            "Epoch 12/20\n",
            "125/125 [==============================] - 3s 24ms/step - loss: 0.5140 - acc: 0.8332 - val_loss: 1.0202 - val_acc: 0.7222\n",
            "Epoch 13/20\n",
            "125/125 [==============================] - 4s 28ms/step - loss: 0.4633 - acc: 0.8510 - val_loss: 0.9426 - val_acc: 0.7434\n",
            "Epoch 14/20\n",
            "125/125 [==============================] - 3s 27ms/step - loss: 0.4273 - acc: 0.8610 - val_loss: 0.9150 - val_acc: 0.7609\n",
            "Epoch 15/20\n",
            "125/125 [==============================] - 3s 25ms/step - loss: 0.3885 - acc: 0.8735 - val_loss: 0.8454 - val_acc: 0.7792\n",
            "Epoch 16/20\n",
            "125/125 [==============================] - 3s 26ms/step - loss: 0.3452 - acc: 0.8865 - val_loss: 0.8956 - val_acc: 0.7694\n",
            "Epoch 17/20\n",
            "125/125 [==============================] - 3s 25ms/step - loss: 0.3106 - acc: 0.8980 - val_loss: 1.1331 - val_acc: 0.7437\n",
            "Epoch 18/20\n",
            "125/125 [==============================] - 4s 31ms/step - loss: 0.2758 - acc: 0.9115 - val_loss: 0.9974 - val_acc: 0.7674\n",
            "Epoch 19/20\n",
            "125/125 [==============================] - 3s 25ms/step - loss: 0.2498 - acc: 0.9161 - val_loss: 1.0286 - val_acc: 0.7659\n",
            "Epoch 20/20\n",
            "125/125 [==============================] - 3s 22ms/step - loss: 0.2196 - acc: 0.9262 - val_loss: 1.1277 - val_acc: 0.7674\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 200)]             0         \n",
            "                                                                 \n",
            " token_and_position_embeddin  (None, 200, 32)          646464    \n",
            " g (TokenAndPositionEmbeddin                                     \n",
            " g)                                                              \n",
            "                                                                 \n",
            " transformer_block (Transfor  (None, 200, 32)          10656     \n",
            " merBlock)                                                       \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 32)               0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 20)                660       \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 20)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 658,200\n",
            "Trainable params: 658,200\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "modeloTransformers.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
        "modeloTransformers.fit(x_train, y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val))\n",
        "print(modeloTransformers.summary())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss, val_accuracy = modeloTransformers.evaluate(x_val, y_val)\n",
        "\n",
        "print(f\"Validation Loss: {val_loss}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIHgSTG_9dZF",
        "outputId": "5b92238b-bd95-4ec4-c155-b0d6f65f3258"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "125/125 [==============================] - 1s 5ms/step - loss: 1.1277 - acc: 0.7674\n",
            "Validation Loss: 1.1276781558990479\n",
            "Validation Accuracy: 0.7674418687820435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Red Neuronal Clásica"
      ],
      "metadata": {
        "id": "rlAA-hMi5-up"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuoAykx55-HA"
      },
      "outputs": [],
      "source": [
        "modeloClasico = keras.models.Sequential()\n",
        "modeloClasico.add(keras.layers.Embedding(20000, 10, input_length=200))\n",
        "modeloClasico.add(keras.layers.Flatten())\n",
        "modeloClasico.add(keras.layers.Dense(512, activation='relu'))\n",
        "modeloClasico.add(keras.layers.Dropout(0.3))\n",
        "modeloClasico.add(keras.layers.Dense(20, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79ooXxc53s2p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fef00df3-aecc-4c0a-e414-fdce15c3a9d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "125/125 [==============================] - 21s 163ms/step - loss: 2.8953 - acc: 0.0972 - val_loss: 2.6500 - val_acc: 0.1568\n",
            "Epoch 2/20\n",
            "125/125 [==============================] - 14s 107ms/step - loss: 2.2651 - acc: 0.2696 - val_loss: 1.9869 - val_acc: 0.3246\n",
            "Epoch 3/20\n",
            "125/125 [==============================] - 11s 92ms/step - loss: 1.5958 - acc: 0.4891 - val_loss: 1.5678 - val_acc: 0.4734\n",
            "Epoch 4/20\n",
            "125/125 [==============================] - 7s 52ms/step - loss: 1.1163 - acc: 0.6531 - val_loss: 1.3060 - val_acc: 0.5459\n",
            "Epoch 5/20\n",
            "125/125 [==============================] - 8s 61ms/step - loss: 0.7759 - acc: 0.7729 - val_loss: 1.1210 - val_acc: 0.6172\n",
            "Epoch 6/20\n",
            "125/125 [==============================] - 5s 37ms/step - loss: 0.5413 - acc: 0.8430 - val_loss: 1.0155 - val_acc: 0.6542\n",
            "Epoch 7/20\n",
            "125/125 [==============================] - 6s 51ms/step - loss: 0.3921 - acc: 0.8908 - val_loss: 0.9986 - val_acc: 0.6714\n",
            "Epoch 8/20\n",
            "125/125 [==============================] - 4s 32ms/step - loss: 0.2909 - acc: 0.9171 - val_loss: 1.0027 - val_acc: 0.6792\n",
            "Epoch 9/20\n",
            "125/125 [==============================] - 3s 27ms/step - loss: 0.2271 - acc: 0.9342 - val_loss: 1.0113 - val_acc: 0.6934\n",
            "Epoch 10/20\n",
            "125/125 [==============================] - 2s 14ms/step - loss: 0.1886 - acc: 0.9441 - val_loss: 1.0184 - val_acc: 0.7009\n",
            "Epoch 11/20\n",
            "125/125 [==============================] - 3s 21ms/step - loss: 0.1590 - acc: 0.9506 - val_loss: 1.0509 - val_acc: 0.7014\n",
            "Epoch 12/20\n",
            "125/125 [==============================] - 2s 15ms/step - loss: 0.1386 - acc: 0.9579 - val_loss: 1.0750 - val_acc: 0.7004\n",
            "Epoch 13/20\n",
            "125/125 [==============================] - 2s 17ms/step - loss: 0.1235 - acc: 0.9594 - val_loss: 1.0753 - val_acc: 0.7092\n",
            "Epoch 14/20\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.1140 - acc: 0.9607 - val_loss: 1.0952 - val_acc: 0.7077\n",
            "Epoch 15/20\n",
            "125/125 [==============================] - 3s 22ms/step - loss: 0.1018 - acc: 0.9631 - val_loss: 1.1228 - val_acc: 0.7049\n",
            "Epoch 16/20\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.1006 - acc: 0.9637 - val_loss: 1.1459 - val_acc: 0.7054\n",
            "Epoch 17/20\n",
            "125/125 [==============================] - 2s 18ms/step - loss: 0.0937 - acc: 0.9649 - val_loss: 1.1704 - val_acc: 0.7077\n",
            "Epoch 18/20\n",
            "125/125 [==============================] - 1s 12ms/step - loss: 0.0898 - acc: 0.9654 - val_loss: 1.1957 - val_acc: 0.7059\n",
            "Epoch 19/20\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.0838 - acc: 0.9666 - val_loss: 1.1943 - val_acc: 0.7067\n",
            "Epoch 20/20\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.0857 - acc: 0.9658 - val_loss: 1.1981 - val_acc: 0.7064\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 200, 10)           200000    \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2000)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 512)               1024512   \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 20)                10260     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,234,772\n",
            "Trainable params: 1,234,772\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "modeloClasico.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "modeloClasico.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
        "modeloClasico.fit(x_train, y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val))\n",
        "print(modeloClasico.summary())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss, val_accuracy = modeloClasico.evaluate(x_val, y_val)\n",
        "\n",
        "print(f\"Validation Loss: {val_loss}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gfx73jA9RLl",
        "outputId": "f518ffdb-8034-4992-ab24-7329fab4c066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "125/125 [==============================] - 0s 2ms/step - loss: 1.1981 - acc: 0.7064\n",
            "Validation Loss: 1.198068618774414\n",
            "Validation Accuracy: 0.7064266204833984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Indica cuál es la precisión del modelo en el conjunto de datos de entrenamiento y en el conjunto de datos de validación. ¿Qué interpretación puedes dar? Haz en este punto un análisis comparativo de los dos modelos ejecutados. "
      ],
      "metadata": {
        "id": "dDxY2KAV2sEb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo Transformers:\n",
        "\n",
        "125/125 [==============================] - 1s 5ms/step - loss: 0.9803 - acc: 0.7882\n",
        "\n",
        "Validation Loss: 0.980324387550354\n",
        "\n",
        "Validation Accuracy: 0.7881970405578613\n",
        "\n",
        "# Modelo Clásico:\n",
        "\n",
        "125/125 [==============================] - 0s 2ms/step - loss: 1.1981 - acc: 0.7064\n",
        "\n",
        "Validation Loss: 1.198068618774414\n",
        "\n",
        "Validation Accuracy: 0.7064266204833984\n"
      ],
      "metadata": {
        "id": "tpWN9PV486CV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.\tEn la parte final del código se hace un análisis cualitativo de la salida. Explica el funcionamiento de este análisis e interpreta los resultados. Haz también en este punto un análisis comparativo de los dos modelos ejecutados. "
      ],
      "metadata": {
        "id": "R18xn4q5EUg4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4x_4eXJVrnX"
      },
      "source": [
        "# Evaluación Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "fgg7KnoioNYc",
        "outputId": "b17fab5e-1d4d-4da8-d534-b6597ee2bf54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 294ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'comp.graphics'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "string_input = keras.Input(shape=(1,), dtype=\"string\")\n",
        "x = vectorizer(string_input)\n",
        "preds = modeloTransformers(x)\n",
        "end_to_end_model = keras.Model(string_input, preds)\n",
        "\n",
        "probabilities = end_to_end_model.predict(\n",
        "    [[\"this message is about computer graphics and 3D modeling\"]]\n",
        ")\n",
        "\n",
        "class_names[np.argmax(probabilities[0])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "R-EXfK6qoSAd",
        "outputId": "6cdf73d8-ed01-409f-d0ca-30fdb23f7d8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 56ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'talk.politics.mideast'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "probabilities = end_to_end_model.predict(\n",
        "    [[\"politics and federal courts law that people understand with politician and elects congressman\"]]\n",
        ")\n",
        "\n",
        "class_names[np.argmax(probabilities[0])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "QByfYDv4rGqv",
        "outputId": "699b1db2-177e-4d6c-b041-1e3aeeacb2ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 34ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'soc.religion.christian'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "probabilities = end_to_end_model.predict(\n",
        "    [[\"we are talking about religion\"]]\n",
        ")\n",
        "\n",
        "class_names[np.argmax(probabilities[0])]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluación Red Clásica"
      ],
      "metadata": {
        "id": "iarpvNyw74uO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "xS9jWNfZ72gx",
        "outputId": "7cabe177-2129-42f2-882f-555f3c0c206f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 117ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'comp.graphics'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "string_input = keras.Input(shape=(1,), dtype=\"string\")\n",
        "x = vectorizer(string_input)\n",
        "preds = modeloClasico(x)\n",
        "end_to_end_model = keras.Model(string_input, preds)\n",
        "\n",
        "probabilities = end_to_end_model.predict(\n",
        "    [[\"this message is about computer graphics and 3D modeling\"]]\n",
        ")\n",
        "\n",
        "class_names[np.argmax(probabilities[0])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "5HB1QIFN72gy",
        "outputId": "cf481118-88d1-4c8b-b1e6-432ba40545f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 31ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'talk.politics.misc'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "probabilities = end_to_end_model.predict(\n",
        "    [[\"politics and federal courts law that people understand with politician and elects congressman\"]]\n",
        ")\n",
        "\n",
        "class_names[np.argmax(probabilities[0])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "1Y_WY21Z72gz",
        "outputId": "b17d0df5-76bd-4a68-83d0-292052d21496"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 47ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'talk.religion.misc'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "probabilities = end_to_end_model.predict(\n",
        "    [[\"we are talking about religion\"]]\n",
        ")\n",
        "\n",
        "class_names[np.argmax(probabilities[0])]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.\tExplica algunas de las limitaciones que puedes encontrar al modelo entrenado."
      ],
      "metadata": {
        "id": "sBDWmGdTEa4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A)\tRequisitos de recursos computacionales y memoria: Los modelos Transformer se caracterizan por requerir una alta capacidad computacional y grandes cantidades de memoria debido a su arquitectura basada en atención y procesamiento paralelo de secuencias largas. Esto puede dificultar su implementación y entrenamiento en dispositivos con recursos limitados, lo que hace necesario el uso de supercomputadoras para desplegar eficientemente este tipo de modelos.\n",
        "\n",
        "B)\tDependencia de datos etiquetados y recursos de entrenamiento: Los modelos Transformer generalmente necesitan una gran cantidad de datos etiquetados para un entrenamiento efectivo. La obtención y etiquetado de conjuntos de datos de alta calidad puede resultar costoso y laborioso, especialmente en dominios especializados o con recursos limitados. Además, la falta de contexto semántico puede agravar este desafío.\n",
        "\n",
        "C)\tLimitaciones en la interpretación: Aunque los modelos Transformer pueden lograr un alto rendimiento en diversas tareas, presentan limitaciones en cuanto a su capacidad para interpretar y explicar los resultados. Debido a la naturaleza de su arquitectura y al procesamiento a nivel de token, comprender cómo se llega a ciertas predicciones o qué características específicas se consideran relevantes puede resultar difícil.\n",
        "\n",
        "D)\tTratamiento de secuencias de longitud variable: Aunque los modelos Transformer pueden manejar secuencias de longitud variable, a menudo requieren técnicas adicionales como el uso de tokens especiales de inicio y fin, así como el relleno de secuencias más cortas para igualar la longitud. Estas técnicas pueden introducir ruido y dificultar el manejo de secuencias extremadamente largas o cortas. Por lo tanto, es necesario aplicar técnicas de procesamiento, como la tokenización o lematización, para obtener información relevante.\n",
        "\n",
        "E)\tCaptura de relaciones a largo plazo: A pesar de los avances logrados por los modelos Transformer en la captura de relaciones a largo plazo en secuencias, aún pueden enfrentar dificultades para capturar dependencias extremadamente prolongadas. En ocasiones, pueden perder información temporal o contextual en secuencias complejas, lo que puede resultar en un desequilibrio de clases y una precisión más baja para las clases menos representadas.\n",
        "\n",
        "F)\tSensibilidad a datos de entrenamiento sesgados: Al igual que otros modelos de aprendizaje automático, los modelos Transformer pueden verse afectados por sesgos presentes en los datos de entrenamiento. Si los datos de entrenamiento están sesgados hacia ciertas clases o contienen sesgos culturales o sociales, el modelo puede aprender y amplificar esos sesgos en sus predicciones.  \n"
      ],
      "metadata": {
        "id": "QDvYfKDBEd0Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9.\t¿Qué sería necesario para que este modelo pueda interpretar textos en español? "
      ],
      "metadata": {
        "id": "qQJx57WKEi-6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para entrenar un modelo Transformer en español, es necesario contar con un conjunto de datos de entrenamiento en español que capture las características y patrones específicos del lenguaje. Estos datos deben reflejar las diferencias lingüísticas entre el español y otros idiomas, como la concordancia gramatical y las reglas de género para los sustantivos.\n",
        "\n",
        "Un enfoque común es utilizar un modelo de lenguaje pre-entrenado en español como punto de partida. Estos modelos han sido previamente entrenados en grandes cantidades de datos en español para aprender la estructura del lenguaje. Ejemplos de modelos de lenguaje pre-entrenados en español incluyen BERT en español, XLM-R en español y otros disponibles a través de la biblioteca de HuggingFace.\n",
        "\n",
        "Además, se puede realizar un ajuste fino (fine-tuning) del modelo utilizando conjuntos de datos más pequeños y específicos en español, adaptándolo a tareas o dominios particulares como clasificación de texto o traducción. Para lograr un procesamiento del lenguaje natural más efectivo en español, también se requieren recursos lingüísticos específicos del español, como modelos de tokenización, etiquetado gramatical, diccionarios de palabras y recursos de lematización. Estos recursos ayudan al modelo a comprender y procesar el español de manera precisa.\n",
        "\n",
        "Es importante llevar a cabo una evaluación continua y realizar ajustes en el modelo a medida que se utilice para interpretar textos en español. Esto implica corregir errores, ajustar los hiperparámetros del modelo y recopilar comentarios de los usuarios para mejorar su capacidad de interpretación en contextos específicos del español.\n"
      ],
      "metadata": {
        "id": "TxtZnSoQElWI"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}