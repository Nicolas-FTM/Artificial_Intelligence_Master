Describe, en menos de 200 palabras, qué es un Word Embeding. En tu descripción deberás además responder a las siguientes cuestiones: ¿qué ventajas aporta sobre técnicas del tipo Bag of Words?, ¿en qué consiste el algoritmo Skip-gram?, ¿por qué decimos que Skip-gram es un modelo auto-supervisado?

Un Word Embedding es una técnica en el procesamiento de lenguaje natural que representa palabras como vectores numéricos en un espacio de baja dimensión. A diferencia del enfoque Bag of Words, los Word Embeddings capturan el significado y la relación contextual entre las palabras. Esto ofrece ventajas, ya que pueden capturar relaciones sintácticas y semánticas, entender analogías y tener una representación más eficiente.

El algoritmo Skip-gram es utilizado para entrenar Word Embeddings. Funciona al predecir el contexto de una palabra en base a un corpus de texto. Se selecciona una palabra y se intenta predecir las palabras vecinas en su contexto. A medida que el modelo se entrena, los vectores de palabras aprendidos capturan el significado y la relación contextual.

Skip-gram es considerado un modelo auto-supervisado, ya que no necesita etiquetas externas para el entrenamiento. Utiliza el corpus de texto sin etiquetas y, a través de la tarea de predicción de contexto, el modelo aprende automáticamente la representación vectorial de las palabras. En resumen, los Word Embeddings son una técnica poderosa que permite representar palabras como vectores numéricos, capturando su significado y contexto, y el algoritmo Skip-gram es utilizado para entrenar estos Word Embeddings de forma auto-supervisada