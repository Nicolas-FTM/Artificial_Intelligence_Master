{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWWKlGjHKmxP"
      },
      "source": [
        "**Universidad Internacional de La Rioja (UNIR) - Máster Universitario en Inteligencia Artificial - Procesamiento del Lenguaje Natural** "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XyRmHptlKmxY"
      },
      "source": [
        "\\***\n",
        "Datos del alumno (Nombre y Apellidos): Nicolás Felipe Trujillo Montero\n",
        "\n",
        "Fecha: 27 de Abril de 2023\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bRI31lrKmxc"
      },
      "source": [
        "<span style=\"font-size: 20pt; font-weight: bold; color: #0098cd;\">Trabajo: Named-Entity Recognition</span>\n",
        "\n",
        "**Objetivos** \n",
        "\n",
        "Con esta actividad se tratará de que el alumno se familiarice con el manejo de la librería spacy, así como con los conceptos básicos de manejo de las técnicas NER\n",
        "\n",
        "**Descripción**\n",
        "\n",
        "En esta actividad debes procesar de forma automática un texto en lenguaje natural para detectar características básicas en el mismo, y para identificar y etiquetar las ocurrencias de conceptos como localización, moneda, empresas, etc.\n",
        "\n",
        "En la primera parte del ejercicio se proporciona un código fuente a través del cual se lee un archivo de texto y se realiza un preprocesado del mismo. En esta parte el alumno tan sólo debe ejecutar y entender el código proporcionado.\n",
        "\n",
        "En la segunda parte del ejercicio se plantean una serie de preguntas que deben ser respondidas por el alumno. Cada pregunta deberá responderse con un fragmento de código fuente que esté acompañado de la explicación correspondiente. Para elaborar el código solicitado, el alumno deberá visitar la documentación de la librería spacy, cuyos enlaces se proporcionarán donde corresponda."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip5A2XZAKmxj"
      },
      "source": [
        "## Parte 1: Carga y Preprocesamiento del texto a analizar\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20GcshBHKmxm"
      },
      "source": [
        "Observa las diferentes librerías que se están importando."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6G4BDCmKmxq"
      },
      "outputs": [],
      "source": [
        "# !python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOU329JLFame"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "S8i0lXTSKmxu"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "import en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTwtNjHpKmxw"
      },
      "source": [
        "La primera libreria, se encarga de usar rutas con sus directorios para operar con archivos, necesario para la realización de esta actividad\n",
        "\n",
        "La segunda libreria es una de las más usadas en el mundo del PLN, y es en la que se centrará la práctica. NLTK ha servido historicamente para primeras aproximaciones en el ámbito de la investigación, aunque Spacy está consiguiendo mayor popularización por sus usos aplicados.\n",
        "Del mismo modo, la siguiente linea de codigo importa una de las funciones necesarias para analisis sintáctico.\n",
        "Por último, [en_core_web_sm](https://spacy.io/models/en#en_core_web_sm) es un pipeline, un modelo pre-entrenado para el procesamiento de texto que en este caso se aplica al inglés.\n",
        "\n",
        "Usando tanto la librería como su pipeline asignado, podemos usar 'ner' como parte de los componentes que integran Spacy. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gsPu3U27Kmxz"
      },
      "outputs": [],
      "source": [
        "nlp = en_core_web_sm.load()\n",
        "file_name = \"barack-obama-speech.txt\"\n",
        "doc = nlp(pathlib.Path(file_name).read_text(encoding=\"utf-8\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TSRhkDkKmx4"
      },
      "source": [
        "\n",
        "En estas lineas de código, cargamos el modelo pre-entrenado, mediante un corpus con un solo documento de texto plano asignado a una variable que será tomada como 'doc'<br>\n",
        "Esta variable doc es un objeto de la clase [Doc](https://spacy.io/api/doc)\n",
        "\n",
        "Con el siguiente método, tomamos el segundo elemento que aparece en nuestro archivo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPYOpLG5Kmx7",
        "outputId": "3d5b4a99-d06d-40ad-bfc6-304ce309fded"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Hello"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Vemos los atributos de la clase\n",
        "# dir(doc)\n",
        "doc.__getitem__(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW1fXI4IKmx_"
      },
      "source": [
        "## Parte 2: Preguntas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rbjg9IMmKmyA"
      },
      "source": [
        "Para responder a cada una de las preguntas planteadas deberás aportar tanto el código fuente con el cual puedes conseguir la respuesta, como una explicación válida de la respuesta y de la forma de obtenerla."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5TOZPz1KmyB"
      },
      "source": [
        "<span style=\"font-size: 14pt; font-weight: bold; color: #0098cd;\">Pregunta 1.</span>\n",
        "<span style=\"font-size: 14pt; font-weight: bold; color: #0098cd;\">¿Cuántas palabras tiene el texto?</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i0gkpCzKmyC",
        "outputId": "9d50b79d-9a03-4bf3-b618-ff39fed70962"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El número de tokens es de 1933\n",
            "El número de palabras 1651\n"
          ]
        }
      ],
      "source": [
        "print(\"El número de tokens es de {}\".format(doc.__len__()))\n",
        "print(\"El número de palabras {}\".format(len(doc.text.split())))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE8KUuMPKmyF"
      },
      "source": [
        "En el primer print, la clase doc tiene una funcion que devuelve el numero de tokens del texto, entendiendose tokens como cualquier componente del texto, es decir, palabras, espacios o signos de puntuación así como información pregramatical como onomatopeyas.\n",
        "\n",
        "En el segundo print, contamos directamente el número de palabras, teniendo como criterio la separación entre espacios. Como el primer item es un entrecomillado, tomará Hello como punto de partida.\n",
        "\n",
        "Sería necesario normalizar el texto, eliminando signos de puntuación.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-Y7GainKmyG"
      },
      "source": [
        "<span style=\"font-size: 14pt; font-weight: bold; color: #0098cd;\">Pregunta 2.</span>\n",
        "<span style=\"font-size: 14pt; font-weight: bold; color: #0098cd;\">¿Cuántas oraciones tiene el texto?</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkzPFjCxKmyJ",
        "outputId": "4cf85a3c-e1ac-4af9-b194-6acbcb95687d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El texto contiene 81 oraciones\n"
          ]
        }
      ],
      "source": [
        "print(\"El texto contiene {} oraciones\".format(len(doc.text.split(\".\"))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z-n4-m7KmyK"
      },
      "source": [
        "Usando el metodo split dividimos mediante delimitador específico, en este caso cada vez que acabe en un punto, aunque habría que distinguir entre aquellas con nucleo verbal de las frases que no las contengan.\n",
        "\n",
        "Del mismo modo, no hemos diferenciado entre otros signos de puntuación como interrogación o exclamación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teyvLv8FKmyM"
      },
      "source": [
        "<span style=\"font-size: 14pt; font-weight: bold; color: #0098cd;\">Pregunta 3.</span>\n",
        "<span style=\"font-size: 14pt; font-weight: bold; color: #0098cd;\">¿Cuál es el número de palabras de la oración más grande? ¿Cual es dicha oración?</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMYPnFgGKmyM",
        "outputId": "cb0e1062-f0e6-4d17-9120-104bf1e5c439"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La oración más grande contiene 58 palabras\n",
            "La oración sería... \n",
            "It drew strength from the not-so-young people who braved the bitter cold and scorching heat to knock on doors of perfect strangers, and from the millions of Americans who volunteered and organized and proved that more than two centuries later a government of the people, by the people, and for the people has not perished from the Earth \n"
          ]
        }
      ],
      "source": [
        "max_sent = \"\"\n",
        "\n",
        "for sent in doc.text.split(\".\"):\n",
        "    if len(max_sent) < len(sent):\n",
        "        max_sent = sent\n",
        "\n",
        "print(\"La oración más grande contiene {} palabras\".format( len(max_sent.split())) )\n",
        "print(\"La oración sería... {} \".format(max_sent))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKgyuP4BKmyN"
      },
      "source": [
        "Mediante un bucle, se ha segmentado en oraciones como en la pregunta anterior para que si la longitud de la oración actual '(len(sent))' es mayor que la longitud de la oración más larga encontrada hasta ahora '(len(max_sent))'<br> Si la oración actual es más larga, se asigna a la variable 'max_sent'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtZ7ycXjKmyP"
      },
      "source": [
        "### Pregunta 4. ¿Cómo puedes acceder al lema, lexema y morfemas de cada token?\n",
        "\n",
        "Recomendación: si no lo has hecho ya, visita la documentación de la clase <i>Token</i>: https://spacy.io/api/token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M56691_mKmyQ",
        "outputId": "8f2893c0-c9ac-45c1-9d18-99d9f0faf69d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La palabra 'founders', cuyo lema es 'founder' asociado al id '4354848742269768285', tendría como lexema 'founders' siendo su morfema 'Number=Plur'\n"
          ]
        }
      ],
      "source": [
        "tkn_example = doc.__getitem__(34)\n",
        "lem = tkn_example.lemma\n",
        "lem2 = tkn_example.lemma_\n",
        "lex = tkn_example.lex.text\n",
        "morph = tkn_example.morph\n",
        "\n",
        "print(\"La palabra '{}', cuyo lema es '{}' asociado al id '{}', tendría como lexema '{}' siendo su morfema '{}'\".format(tkn_example, lem2, lem, lex, morph))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2gSXnJrKmyQ"
      },
      "source": [
        "\n",
        "En la lingüística, un lema es la raíz de una palabra, la unidad mínima de significado.\n",
        "\n",
        "El lexema consiste en la palabra junto a sus morfemas asociados, donde el lema se convierte en diferentes tipos de palabras como sustantivos o adjetivos, siendo en el español obligatorio para la concordancía gramátical.\n",
        "\n",
        "\n",
        "En este caso, al lema 'founder' se le añade un morfema '-s' de plural obteniendo el lexema 'founders'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pj4_oxh8KmyS"
      },
      "source": [
        "<span style=\"font-size: 14pt; font-weight: bold; color: #0098cd;\">Pregunta 5.</span>\n",
        "<span style=\"font-size: 14pt; font-weight: bold; color: #0098cd;\">¿Cómo puedes identificar/eliminar las stop words?</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZkVppU4KmyS",
        "outputId": "2e620c88-d476-498f-e33d-6075f28c0092"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¿Es 'founders' una stop word? No\n"
          ]
        }
      ],
      "source": [
        "print(\"¿Es '{}' una stop word? {}\".format(tkn_example.text, \"Sí\" if tkn_example.is_stop else \"No\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgieMUTHKmyT"
      },
      "source": [
        "\n",
        "En programación orientada a objetos, una instancia es un objeto creado a partir de una clase. Esta clase define los atributos y métodos que tendrán los objetos creados a partir de ella.\n",
        "Spacy nos permite tomar cada instancia de la clase Token con un atributo asociado, en este caso indicando si el elemento es una stop word.\n",
        "\n",
        "Podríamos crear un bucle donde se recorran las stop words del texto, eliminándolas. En la biblioteca de Spacy, se puede obtener un conjunto predefinido de stop words para ello:\n",
        "\n",
        "    *from spacy.lang.en.stop_words import STOP_WORDS*\n",
        "\n",
        "    *nlp = spacy.load(\"en_core_web_sm\")*\n",
        "\n",
        "    *stopwords = list(STOP_WORDS)*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWN-IkuEKmyT"
      },
      "source": [
        "<span style=\"font-size: 14pt; font-weight: bold; color: #0098cd;\">Pregunta 6.</span>\n",
        "<span style=\"font-size: 14pt; font-weight: bold; color: #0098cd;\">¿Qué atributo del token contiene la etiqueta NER?</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6pBrYOYKmyT",
        "outputId": "fc946309-da59-4d78-9860-43b8ce978cd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['of', 'States', 'Chicago', 'Des', 'Moines', 'Afghanistan', 'Iraq', 'Birmingham', 'Atlanta', 'Concord', 'America', 'the', 'Charleston', 'Selma', 'Berlin', 'United', 'Washington']\n"
          ]
        }
      ],
      "source": [
        "# displacy.render(doc, style=\"ent\")\n",
        "estados = []\n",
        "\n",
        "for tkn in doc.__iter__():\n",
        "    if tkn.ent_type_ == \"GPE\":\n",
        "        estados.append(tkn.text)\n",
        "\n",
        "print(list(set(estados)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87M2Q35GKmyU"
      },
      "source": [
        "Con la primera linea puesta en comentarios, desplegamos visualmente NER.\n",
        "\n",
        "Creamos una lista donde añadimos todos los que contengan GPE, que en este caso corresponden a territorios administrativos como paises o estados de EEUU.\n",
        "\n",
        "Como hemos visto en el caso anterior, se podrían eliminar stop words como 'of' o 'the' si fuese necesario para nuestro procesamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avUh8vdBKmyU"
      },
      "source": [
        "<span style=\"font-size: 14pt; font-weight: bold; color: #0098cd;\">Pregunta 7.</span>\n",
        "<span style=\"font-size: 14pt; font-weight: bold; color: #0098cd;\">¿Qué entidades soporta Spacy?, ¿Qué significa cada una?</span>\n",
        "\n",
        "<b>Nota</b>: Debes escribir el código que liste las entidades disponibles y la explicación de las mismas. El listado sin código se considerará respuesta incompleta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbfYI5KkKmyV",
        "outputId": "2740c608-c754-43f2-e812-ddb5cb76b365"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La entidad CARDINAL incluye 'Numerals that do not fall under another type'\n",
            "La entidad DATE incluye 'Absolute or relative dates or periods'\n",
            "La entidad EVENT incluye 'Named hurricanes, battles, wars, sports events, etc.'\n",
            "La entidad FAC incluye 'Buildings, airports, highways, bridges, etc.'\n",
            "La entidad GPE incluye 'Countries, cities, states'\n",
            "La entidad LANGUAGE incluye 'Any named language'\n",
            "La entidad LAW incluye 'Named documents made into laws.'\n",
            "La entidad LOC incluye 'Non-GPE locations, mountain ranges, bodies of water'\n",
            "La entidad MONEY incluye 'Monetary values, including unit'\n",
            "La entidad NORP incluye 'Nationalities or religious or political groups'\n",
            "La entidad ORDINAL incluye '\"first\", \"second\", etc.'\n",
            "La entidad ORG incluye 'Companies, agencies, institutions, etc.'\n",
            "La entidad PERCENT incluye 'Percentage, including \"%\"'\n",
            "La entidad PERSON incluye 'People, including fictional'\n",
            "La entidad PRODUCT incluye 'Objects, vehicles, foods, etc. (not services)'\n",
            "La entidad QUANTITY incluye 'Measurements, as of weight or distance'\n",
            "La entidad TIME incluye 'Times smaller than a day'\n",
            "La entidad WORK_OF_ART incluye 'Titles of books, songs, etc.'\n"
          ]
        }
      ],
      "source": [
        "for _typeNER in nlp.pipe_labels['ner']:\n",
        "        print(\"La entidad {} incluye '{}'\".format(_typeNER, spacy.explain(_typeNER)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB_tqnzgKmyW"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Imprimos en lenguaje natural cada entidad que puede soportar Spacy, junto a su descripción."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms4r4gPMKmyX"
      },
      "source": [
        "<span style=\"font-size: 14pt; font-weight: bold; color: #0098cd;\">Pregunta 8.</span>\n",
        "<span style=\"font-size: 14pt; font-weight: bold; color: #0098cd;\">¿Qué entidades diferentes son reconocidas en el texto?, ¿cuántas hay de cada tipo?</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvLq95YsKmyY",
        "outputId": "32b5bbfe-423c-42c0-fc11-1501a79ff9ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'GPE': 28, 'TIME': 19, 'ORDINAL': 1, 'NORP': 12, 'MONEY': 7, 'CARDINAL': 7, 'DATE': 26, 'LOC': 2, 'FAC': 1, 'ORG': 10, 'PERSON': 5}\n"
          ]
        }
      ],
      "source": [
        "# displacy.render(doc, style=\"ent\")\n",
        "\n",
        "NER = {}\n",
        "\n",
        "for tkn in doc.__iter__():\n",
        "    if tkn.ent_type_ != \"\":\n",
        "        try:\n",
        "            NER[tkn.ent_type_] = NER[tkn.ent_type_] + 1\n",
        "        except:\n",
        "            NER[tkn.ent_type_] = 0\n",
        "print(NER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gHnRSKkKmyZ"
      },
      "source": [
        "Meidante un bucle todos los tokens, comprobemos cada entidad reconocida como NER, contabilizando cuantos items habría de cada una"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c480m-1LKmya"
      },
      "source": [
        "<span style=\"font-size: 14pt; font-weight: bold; color: #0098cd;\">Pregunta 9.</span>\n",
        "<span style=\"font-size: 14pt; font-weight: bold; color: #0098cd;\">Explica con tus palabras qué es el código IOB para el reconocimiento de entidades. Pon un ejemplo, sacado del texto, de una etiqueta de un único token y una etiqueta compuesta por varios tokens.</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Dy1qb-vTXbT"
      },
      "source": [
        "Usando IOB, podemos etiquetar palabras en un corpus para aplicarse en procesos de NER, con lo que podemos entrenar nuestros propios modelos.\n",
        "\n",
        "Para ello tendríamos tres etiquetas definidas:\n",
        "\n",
        "*   B = Beginning, siendo la primera palabra que defina la entidad\n",
        "*   I = Inside, resto de palabras dentro de la misma entidad\n",
        "*   O = Outside, palabras fuera de una entidad\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Tendríamos...\n",
        "\n",
        "*'Hello, Chicago'* <br>\n",
        "\n",
        "Hello - O <br>\n",
        "Chicago - B-GPE <br>\n",
        "\n",
        "<br>\n",
        "\n",
        "*'We are, and always will be, the United States of America.'* <br>\n",
        "\n",
        "We - O <br>\n",
        "are - O <br>\n",
        "and - O <br>\n",
        "always - O <br>\n",
        "will - O <br>\n",
        "be - O <br>\n",
        "the - O <br>\n",
        "United - B-GPE <br>\n",
        "States - I-GPE <br>\n",
        "of - I-GPE <br>\n",
        "America - I-GPE <br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IcxN2JQKmyc"
      },
      "source": [
        "<b>Incluye aquí, debajo de la línea, la explicación de tu respuesta</b>\n",
        "<hr>\n",
        " "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
